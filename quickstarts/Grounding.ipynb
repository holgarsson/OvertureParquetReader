{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_lgX9omPXF-"
      },
      "source": [
        "## Gemini API: Getting started with information grounding for Gemini models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkR4fWudrHCs"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Grounding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKKNfXWrHgs"
      },
      "source": [
        "In this notebook you will learn how to use information grounding with [Gemini models](https://ai.google.dev/gemini-api/docs/models/).\n",
        "\n",
        "Information grounding is the process of connecting these models to specific, verifiable information sources to enhance the accuracy, relevance, and factual correctness of their responses. While LLMs are trained on vast amounts of data, this knowledge can be general, outdated, or lack specific context for particular tasks or domains. Grounding helps to bridge this gap by providing the LLM with access to curated, up-to-date information.\n",
        "\n",
        "Here you will experiment with:\n",
        "- Grounding information using <a href=\"#search_grounding\">Google Search grounding</a>\n",
        "- Adding <a href=\"#yt_links\">YouTube links</a> to gather context information to your prompt\n",
        "- Using <a href=\"#url_context\">URL context</a> to include website, pdf or image URLs as context to your prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKu1tRBrQ7xj"
      },
      "source": [
        "## Set up the SDK and the client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIWKUlPqP5NK"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "This guide uses the [`google-genai`](https://pypi.org/project/google-genai) Python SDK to connect to the Gemini models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6Fr84vJuPSHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1279ff7-1963-4e47-d8a2-9c4a53644307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.6/245.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q -U \"google-genai>=1.16.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a503bnWNQoCL"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhKXgMSNQrrV"
      },
      "source": [
        "### Select model and initialize SDK client\n",
        "\n",
        "Select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RjvgYmdLQd5s"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = 'AIzaSyC3XrUYLL0PVdA9hALxcf3q8RLlk3lV0y0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C75s1LR9QmOz"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite\",\"gemini-2.5-flash\",\"gemini-2.5-pro\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FHIcazUO0-xU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "9aa8c4e7-d72f-4676-9d6c-0c4e031be777"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Response**:\n The latest Indian Premier League (IPL) match was the final of the 2025 season, played between Royal Challengers Bengaluru (RCB) and Punjab Kings (PBKS) at the Narendra Modi Stadium in Ahmedabad. Royal Challengers Bengaluru won the match by 6 runs, securing their first IPL title."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search Query: ['latest Indian Premier League match and winner', 'who won the last IPL match']\n",
            "Search Pages: jagranjosh.com, indiatimes.com, cricbuzz.com\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2mURlMZNw6djX90FptxU7MMvvFTNPYuiVRPpqVtas2S5AoAWkLmFngkup6IyNk1YajnoZGvq6LDCP7ybJonLbBjEwdBH7yvxVmf3rzpIgVLlUXPH0CGmFMPn2Tz7YvaOqRvLsYFroFE6FgcdzderH03aJ2GX5HeOwD2zkcrHI13zgGN569YD-gwVRsUA7iroq5MquCEgayM3cgUt-\">who won the last IPL match</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGs-ngtyL0VBj8fPz2086emXmk2djjYbgYDaMo9hGvfGJYJ4xJDL9__m7ZJTYufpXa90dAtijNFMIY3FVc8WomK6IcDfZYz-TleYLZhyOXWlQoxoO-rUn4lBemPXqZE32nlEag2Qtuoxsx-dhGbGsMVL8MBAJa0WpG1x-CjFogEcKbCJykWmORhoDIcBzXe1nDodBr0La_DrYpw62KlSocuCV3QYEW-SgePZV4rrSt0uA==\">latest Indian Premier League match and winner</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='What was the latest Indian Premier League match and who won?',\n",
        "    config={\"tools\": [{\"google_search\": {}}]},\n",
        ")\n",
        "\n",
        "# print the response\n",
        "display(Markdown(f\"**Response**:\\n {response.text}\"))\n",
        "# print the search details\n",
        "print(f\"Search Query: {response.candidates[0].grounding_metadata.web_search_queries}\")\n",
        "# urls used for grounding\n",
        "print(f\"Search Pages: {', '.join([site.web.title for site in response.candidates[0].grounding_metadata.grounding_chunks])}\")\n",
        "\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wROLHEYLLBHX"
      },
      "source": [
        "You can see that running the same prompt without search grounding gives you outdated information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EdUkQ40cKaGX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "4ec1b572-0dfa-4a35-c9d3-d4e893c71ea3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The latest Indian Premier League match was the **final of the IPL 2024 season**, played on **May 26, 2024**.\n\n**Kolkata Knight Riders (KKR)** played against **Sunrisers Hyderabad (SRH)**.\n\n**Kolkata Knight Riders (KKR) won the match by 8 wickets**, securing their third IPL title."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='What was the latest Indian Premier League match and who won?',\n",
        ")\n",
        "\n",
        "# print the response\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE6Ft1wxSxO_"
      },
      "source": [
        "For more examples, please refer to the [dedicated notebook](./Search_Grounding.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XfNrFR7j6F6"
      },
      "source": [
        "## Grounding with YouTube links\n",
        "\n",
        "<a name=\"yt_links\"></a>\n",
        "\n",
        "you can directly include a public YouTube URL in your prompt. The Gemini models will then process the video content to perform tasks like summarization and answering questions about the content.\n",
        "\n",
        "This capability leverages Gemini's multimodal understanding, allowing it to analyze and interpret video data alongside any text prompts provided.\n",
        "\n",
        "You do need to explicitly declare the video URL you want the model to process as part of the contents of the request using a `FileData` part. Here a simple interaction where you ask the model to summarize a YouTube video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "akVTribOkgT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "fd458214-6238-4cda-f7a9-bb1a8e6d3246"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This video introduces \"Gemma Chess,\" demonstrating how Google's Gemma AI model can bring a new dimension to chess beyond traditional engines that simply calculate optimal moves.\n\nThe speaker, Ju-yeong Ji from Google DeepMind, explains that Gemma, as a language model, excels at understanding and creating text based on patterns. This capability is applied to chess in three key ways:\n\n1.  **Explainer/Analyzer:** Gemma can analyze a chess game (e.g., Kasparov vs. Deep Blue) and identify the \"most interesting move.\" Crucially, it provides human-readable explanations of *why* a move is significant, detailing strategic concepts like \"sacrifice and initiative,\" potential calculation errors, or psychological impact on players, rather than just raw numerical evaluations.\n2.  **Storyteller:** Gemma can transform a sequence of chess moves into an engaging short story, narrating the flow of the game, including contextual details, player intentions, and the drama of the match. This brings games to life in a way traditional move lists cannot.\n3.  **Learning Assistant:** Gemma acts as a personalized chess tutor, capable of explaining complex chess concepts (like the \"Sicilian Defense\" or a \"passed pawn\") in simple terms, tailoring the explanation to the user's skill level (beginner, intermediate, advanced), and even in different languages.\n\nThe underlying technology combines traditional chess engine computation (for finding optimal moves) with Gemma's linguistic abilities via \"function calls.\" This allows Gemma to retrieve precise chess information and then interpret and explain it in a human-like, intuitive manner, making chess learning and analysis more accessible and enriching."
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "yt_link = \"https://www.youtube.com/watch?v=XV1kOFo1C8M\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents= types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Summarize this video.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(\n",
        "                    file_uri=yt_link\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wtt-6tQiYkD4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR7sQVlxy8Yr"
      },
      "source": [
        "But you can also use the link as the source of truth for your request. In this example, you will first ask how Gemma models can help on chess games:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mDMScex1It5"
      },
      "source": [
        "## Use Google Search grounding\n",
        "\n",
        "<a name=\"search_grounding\"></a>\n",
        "\n",
        "Google Search grounding is particularly useful for queries that require current information or external knowledge. Using Google Search, Gemini can access nearly real-time information and better responses.\n",
        "\n",
        "To enable Google Search, simply add the `google_search` tool in the `generate_content`'s `config` that way:\n",
        "```\n",
        "    config={\n",
        "      \"tools\": [\n",
        "        {\n",
        "          \"google_search\": {}\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UTH4DqBAzx3H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "55d4eb63-0ef4-4c03-9800-bf907f0f4c01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The team behind Duke Nukem 3D could have achieved significantly greater success by demonstrating more disciplined project management and resisting the urge for perpetual \"feature creep.\" After the original game's immense popularity, instead of committing to a never-ending development cycle for *Duke Nukem Forever* that saw multiple engine changes, constant design revisions, and attempts to incorporate every new industry innovation, they should have focused on a clearly defined, achievable scope. Releasing a solid, albeit imperfect, sequel within a reasonable timeframe would have capitalized on the massive momentum and fan demand, generating continued revenue and maintaining brand relevance.\n\nThis lack of focus and extended development not only bled financial resources but also severely impacted team morale, leading to significant talent departures and ultimately the downfall of 3D Realms as a primary developer. A more successful approach would have involved an iterative development model, delivering smaller, consistent updates or sequels to keep players engaged and provide regular milestones for the development team. Prioritizing timely delivery over an elusive perfection would have allowed the original creators to sustain their creative output and commercial viability, rather than having their most anticipated project become an industry cautionary tale."
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "yt_link = \"https://www.youtube.com/watch?v=9q3RaXWvSRk\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"In 2 paragraph, how could the team behind Duke Nukem 3D have been more successful?\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri=yt_link)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHhdfKqLz_D6"
      },
      "source": [
        "Now your answer is more insightful for the topic you want, using the knowledge shared on the video and not necessarily available on the model knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKBPhxA-0RiT"
      },
      "source": [
        "## Grounding information using URL context\n",
        "\n",
        "<a name=\"url_context\"></a>\n",
        "\n",
        "The URL Context tool empowers Gemini models to directly access and process content from specific web page URLs you provide within your API requests. This is incredibly interesting because it allows your applications to dynamically interact with live web information without needing you to manually pre-process and feed that content to the model.\n",
        "\n",
        "URL Context is effective because it allows the models to base its responses and analysis directly on the content of the designated web pages. Instead of relying solely on its general training data or broad web searches (which are also valuable grounding tools), URL Context anchors the model's understanding to the specific information present at those URLs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7GrocBgYgrp"
      },
      "source": [
        "### Process website URLs\n",
        "\n",
        "If you want Gemini to specifically ground its answers thanks to the content of a specific website, just add the urls in your prompt and enable the tool by adding it to your config:\n",
        "```\n",
        "config = {\n",
        "  \"tools\": [\n",
        "    {\n",
        "      \"url_context\": {}\n",
        "    }\n",
        "  ],\n",
        "}\n",
        "```\n",
        "\n",
        "You can add up to 20 links in your prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eOXM1Fh2D9Ai",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2f18103f-3ca4-4f22-8a32-ae61de8ee667"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I am unable to access the full content of the YouTube video at https://www.youtube.com/watch?v=9q3RaXWvSRk because \"This content isn't available\" through the browsing tool. Therefore, I cannot provide the key differences between successful and less successful development teams specifically based on that video.\n\nIf you have another resource or would like me to provide a general answer based on common knowledge regarding successful versus less successful development teams, please let me know."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Based on https://www.youtube.com/watch?v=9q3RaXWvSRk, What at the key differences between a successful development team versus\n",
        "  a less successful one where Duke Nukem 3D was too slow to release.\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"tools\": [{\"url_context\": {}}],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    contents=[prompt],\n",
        "    model=MODEL_ID,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "from textwrap import dedent\n",
        "\n",
        "# ---- Pick a valid model name ----\n",
        "MODEL_ID = \"gemini-2.5-flash\"   # or \"gemini-2.5-pro\"\n",
        "URL = \"https://tarv.fo/\"\n",
        "\n",
        "PROMPT = dedent(f\"\"\"\n",
        "You are a precise extractor. Use the URL context tool to read the website:\n",
        "{URL}\n",
        "\n",
        "Return a SINGLE JSON object. No prose, no markdown, no code fences.\n",
        "If a value is missing on the site, set it to null. Keep text concise.\n",
        "\n",
        "PRICE & CURRENCY\n",
        "- price: may include currency symbol (e.g., \"kr 125\", \"€12.50\")\n",
        "- price_clean: numeric string only, '.' as decimal, max 2 decimals\n",
        "  examples: \"$12.50\" → \"12.50\", \"€15,90\" → \"15.90\", \"kr 125\" → \"125.00\", \"¥1,200\" → \"1200.00\"\n",
        "- currency: 3-letter ISO if price exists (map symbols: $→USD, €→EUR, £→GBP, ¥→JPY, kr→DKK/SEK/NOK by context).\n",
        "  On .fo default to DKK unless the page clearly shows another currency; else null.\n",
        "\n",
        "CLASSIFICATION STRINGS\n",
        "- item_type: starters/mains/desserts/sides/wine/beer/cocktails/non_alcoholic/other\n",
        "- category: primary category name (use \"other\" only if unavoidable)\n",
        "- item_subtype: most specific subtype; avoid \"other\".\n",
        "If any of item_type/category/item_subtype is \"other\", set classification_reason (>= 8 chars).\n",
        "\n",
        "SCHEMA (return exactly this shape):\n",
        "{{\n",
        "  \"restaurant_data\": {{\n",
        "    \"restaurant_name\": string,\n",
        "    \"opening_hours\": string|null,\n",
        "    \"address_and_location\": string|null,\n",
        "    \"contact_information\": string|null,\n",
        "    \"reservation_details\": string|null,\n",
        "    \"online_ordering\": string|null,\n",
        "    \"family_friendly\": string|null,\n",
        "    \"parking_and_accessibility\": string|null,\n",
        "    \"dress_code_or_etiquette\": string|null,\n",
        "    \"chain\": string|null,\n",
        "    \"lunch_combo\": string|null,\n",
        "    \"a_la_carte_or_set_menu\": string|null,\n",
        "    \"parent_chain_url\": string|null,\n",
        "\n",
        "    \"representative_dish\": {{\n",
        "      \"name\": string,\n",
        "      \"price\": string|null,\n",
        "      \"price_clean\": string|null,\n",
        "      \"currency\": string|null,\n",
        "      \"description\": string|null,\n",
        "      \"type\": string|null,\n",
        "      \"reason\": string|null\n",
        "    }},\n",
        "\n",
        "    \"menu_and_prices\": [\n",
        "      {{\n",
        "        \"name\": string,\n",
        "        \"price\": string|null,\n",
        "        \"price_clean\": string|null,\n",
        "        \"currency\": string|null,\n",
        "        \"description\": string|null,\n",
        "        \"item_type\": string,\n",
        "        \"category\": string,\n",
        "        \"time_of_day\": string|null,\n",
        "        \"reasoning\": string|null,\n",
        "        \"item_subtype\": string,\n",
        "        \"classification_reason\": string|null\n",
        "      }}\n",
        "    ]|null,\n",
        "\n",
        "    \"socials\": [{{ \"platform\": string, \"url\": string }}]|null,\n",
        "    \"external_menu_urls\": [{{ \"url\": string }}]|null\n",
        "  }},\n",
        "\n",
        "  \"extraction_metadata\": {{\n",
        "    \"extraction_timestamp\": string|null,\n",
        "    \"pages_visited\": [string]|null,\n",
        "    \"total_items_found\": integer|null,\n",
        "    \"extraction_challenges\": [string]|null,\n",
        "    \"navigation_decisions\": [{{ \"url\": string|null, \"reason\": string|null }}]|null\n",
        "  }}|null\n",
        "}}\n",
        "\"\"\").strip()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[PROMPT],\n",
        "    config={\"tools\": [{\"url_context\": {}}]}  # URL grounding\n",
        "    # If supported in your SDK, JSON mode helps:\n",
        "    # , generation_config={\"response_mime_type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "Markdown(response.text)\n",
        "#raw = getattr(response, \"text\", None) or str(response)\n",
        "\n",
        "#def extract_json(s: str):\n",
        "#    m = re.search(r\"\\{[\\s\\S]*\\}\\s*$\", s)\n",
        "#    return json.loads(m.group(0)) if m else json.loads(s)\n",
        "#\n",
        "#data = extract_json(raw)\n",
        "#print(json.dumps(data, ensure_ascii=False, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "-giexFQief0n",
        "outputId": "6943a7e4-b34a-42a8-c502-1cd80b8034cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 2 (char 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1826797824.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1826797824.py\u001b[0m in \u001b[0;36mextract_json\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\{[\\s\\S]*\\}\\s*$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPPCD5f2MSIx"
      },
      "source": [
        "You can see the status of the retrival using `url_context_metadata`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "V5kKeAX5MUsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215a2ec6-1f28-4f62-888e-1a6121353562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "url_metadata=[UrlMetadata(\n",
            "  retrieved_url='https://ai.google.dev/gemini-api/docs/models',\n",
            "  url_retrieval_status=<UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: 'URL_RETRIEVAL_STATUS_SUCCESS'>\n",
            ")]\n"
          ]
        }
      ],
      "source": [
        "# get URLs retrieved for context\n",
        "print(response.candidates[0].url_context_metadata)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS2xyoMPY8u-"
      },
      "source": [
        "### Add PDFs by URL\n",
        "\n",
        "Gemini can also process PDFs from an URL. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SeMZX5C5sLe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "70210a13-ac9d-40c0-b2ab-7f64f7c68328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:17: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-717595382.py:17: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  display(Markdown(response.text.replace('$','\\$')))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Alphabet Inc. announced its financial results for the second quarter ended June 30, 2025. The company reported consolidated revenues of \\$96.4 billion, a 14% increase year-over-year (13% in constant currency), driven by strong performance across Google Search & other, YouTube ads, Google subscriptions, platforms, and devices, and Google Cloud.\n\nKey financial highlights for Q2 2025 include:\n*   **Google Services revenues** increased by 12% to \\$82.5 billion.\n*   **Google Cloud revenues** grew by 32% to \\$13.6 billion, primarily due to growth in Google Cloud Platform (GCP) products, AI Infrastructure, and Generative AI Solutions. Google Cloud's annual revenue run-rate now exceeds \\$50 billion.\n*   **Total operating income** increased by 14%, with an operating margin of 32.4%.\n*   **Net income** rose by 19%, and diluted **EPS** increased by 22% to \\$2.31.\n\nSundar Pichai, CEO of Alphabet, highlighted the company's robust growth and leadership in AI, noting that AI is positively impacting every part of the business, including Search (which saw double-digit revenue growth with features like AI Overviews and AI Mode performing well) and YouTube. In response to strong demand for Cloud products and services, Alphabet is increasing its capital expenditures in 2025 to approximately \\$85 billion.\n\nThe report also provides detailed breakdowns of revenues, traffic acquisition costs, segment operating results (Google Services, Google Cloud, and Other Bets), and balance sheet and cash flow statements. In May 2025, Alphabet issued \\$12.5 billion in fixed-rate senior unsecured notes for general corporate purposes. The document concludes with information on their investor relations website, webcast details, forward-looking statements, and explanations of non-GAAP financial measures."
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Can you give me an overview of the content of this pdf?\n",
        "  https://abc.xyz/assets/cc/27/3ada14014efbadd7a58472f1f3f4/2025q2-alphabet-earnings-release.pdf\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"tools\": [{\"url_context\": {}}],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    contents=[prompt],\n",
        "    model=MODEL_ID,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text.replace('$','\\$')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAkMWXwAxiaT"
      },
      "source": [
        "### Add images by URL\n",
        "\n",
        "Gemini can also process images from an URL. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HPNxQYkx8WJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "851cb78c-5a77-4076-fe9c-4a0528e1e5bc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The image provided is a diagram of a trombone with numbered parts. Since the image does not contain embedded text labels, I cannot directly extract the names of the parts from the browsed content. However, based on general knowledge of trombone anatomy, here are the likely names of the numbered parts in French:\n\n1.  **Contrepoids** (Counterweight)\n2.  **Embouchure** (Mouthpiece)\n3.  **Pavillon** (Bell)\n4.  **Clé d'eau** (Water key or Spit valve)\n5.  **Coulisse extérieure** (Outer slide)\n6.  **Pompe d'accord** (Main tuning slide)\n7.  **Branche d'embouchure** (Leadpipe or Mouthpipe)\n8.  **Coulisse intérieure** (Inner slide)"
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Can you help me name of the numbered parts of that instrument, in French?\n",
        "  https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Trombone.svg/960px-Trombone.svg.png\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    \"tools\": [{\"url_context\": {}}],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    contents=[prompt],\n",
        "    model=MODEL_ID,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHs8FDfSsjt2"
      },
      "source": [
        "## Mix Search grounding and URL context\n",
        "\n",
        "The different tools can also be use in conjunction by adding them both to the config. It's a good way to steer Gemini in the right direction and then let it do its magic using search grounding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wEOpBpbssjbD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "outputId": "bbcdac6c-524d-4277-cb3a-e3f128808456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:20: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-110788399.py:20: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  display(Markdown(response.text.replace('$','\\$')))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\nAlphabet Inc. announced strong financial results for the second quarter of 2025, ending June 30, 2025. Consolidated revenues increased by 14% year-over-year (13% in constant currency) to \\$96.4 billion, driven by robust performance across Google Search & other, YouTube ads, Google subscriptions, platforms, and devices, and Google Cloud, all of which achieved double-digit growth.\n\nKey highlights include:\n*   **Google Services revenues** rose by 12% to \\$82.5 billion.\n*   **Google Cloud revenues** surged by 32% to \\$13.6 billion, primarily fueled by growth in Google Cloud Platform (GCP) across core GCP products, AI Infrastructure, and Generative AI Solutions. Google Cloud's annual revenue run-rate now exceeds \\$50 billion.\n*   **Total operating income** increased by 14%, with an operating margin of 32.4%.\n*   **Net income** saw a 19% increase, and **Diluted Earnings Per Share (EPS)** grew by 22% to \\$2.31.\n*   Alphabet plans to increase its capital expenditures in 2025 to approximately \\$85 billion, reflecting strong and growing demand for its Cloud products and services.\n*   CEO Sundar Pichai highlighted the company's leadership in AI, stating that AI is positively impacting every part of the business, driving strong momentum, and that new features like AI Overviews and AI Mode in Search are performing well.\n\nNow, I will search for financial analysts' reactions.Alphabet Inc.'s second quarter 2025 earnings release details a robust financial performance with consolidated revenues increasing by 14% year-over-year to \\$96.4 billion. This growth was propelled by strong double-digit increases across Google Search & other, YouTube ads, Google subscriptions, platforms, and devices, and Google Cloud. Google Services revenues grew by 12% to \\$82.5 billion, while Google Cloud revenues saw a significant 32% increase, reaching \\$13.6 billion, largely due to growth in Google Cloud Platform (GCP) products, AI Infrastructure, and Generative AI Solutions. The company reported a 14% rise in total operating income, with an operating margin of 32.4%, and net income increased by 19%, leading to a 22% increase in diluted EPS to \\$2.31. Alphabet also announced plans to raise its 2025 capital expenditures to approximately \\$85 billion, primarily for AI and cloud infrastructure investments. CEO Sundar Pichai emphasized the positive impact of AI across all business segments and the strong performance of new AI features in Search. Additionally, the company initiated a quarterly dividend of \\$0.21 per share and approved a new \\$70 billion share buyback program.\n\nThe reaction from main financial analysts to Alphabet's Q2 2025 earnings has been largely bullish and positive. The company's revenues and diluted EPS comfortably exceeded analyst expectations. Analysts view advertising and cloud activities as key growth catalysts, with significant momentum attributed to AI advancements, especially in Google Cloud and enhanced ad targeting. Although the increased capital expenditure forecast to \\$85 billion for 2025 led to a slight post-market dip in the stock, this spending is widely seen as a strategic investment to solidify Alphabet's leadership in AI and cloud infrastructure, ensuring long-term growth. The introduction of a quarterly dividend and a substantial share buyback program has been met with positive sentiment, signaling a new era of capital returns and reinforcing investor confidence. While the market remains attentive to an upcoming antitrust ruling concerning Google's Chrome browser, the overarching trend indicates strong confidence in Alphabet's strategic direction and its ability to leverage AI for sustained growth."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFSR09FPK_opqKcUsv64eNV-VQoYkqfh8z4NhLegVQL5Lwf2xHpA6oFtXMHalKI4kbpcnNLyAToF2QzByUFD1ycYWhLxveRiIiBgHUwH9p2yeDOYcArmvsS0tbh_2U2DBiSYlMtYxK0i0Up-gUbwt-3rp-bOhW-2lkiSS73U_GsJccdLRjIAjEbgOjUlGJzdGvOyDH_hp0UiDCpms0G8tjv9tcKZgFVwZii62cdt6hr9ZE-0KnDEhvUpSOtpq3P\">financial analysts reaction Alphabet Q2 2025 earnings trend</a>\n",
              "  </div>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  Can you give me an overview of the content of this pdf?\n",
        "  https://abc.xyz/assets/cc/27/3ada14014efbadd7a58472f1f3f4/2025q2-alphabet-earnings-release.pdf\n",
        "  Search on the web for the reaction of the main financial analysts, what's the trend?\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "  \"tools\": [\n",
        "      {\"url_context\": {}},\n",
        "      {\"google_search\": {}}\n",
        "  ],\n",
        "}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  contents=[prompt],\n",
        "  model=MODEL_ID,\n",
        "  config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text.replace('$','\\$')))\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOt32shZaEXj"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "<a name=\"next_steps\"></a>\n",
        "\n",
        "* For more details about using Google Search grounding, check out the [Search Grounding cookbook](./Search_Grounding.ipynb).\n",
        "* If you are looking for another scenarios using videos, take a look at the [Video understanding cookbook](./Video_understanding.ipynb).\n",
        "\n",
        "Also check the other Gemini capabilities that you can find in the [Gemini quickstarts](https://github.com/google-gemini/cookbook/tree/main/quickstarts/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "Grounding.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}